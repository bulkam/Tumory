{
 "metadata": {
  "name": "",
  "signature": "sha256:16c8842ee73ba794477c1dc29883d11356c4ab7695de8399cd09143130925290"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import classifier as clas\n",
      "import copy\n",
      "import numpy as np\n",
      "import os"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svm = clas.Classifier()\n",
      "svm.create_training_data()\n",
      "#svm.train()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scorings = ['accuracy', \n",
      "            'average_precision',\n",
      "            'f1', 'f1_macro', 'f1_micro', 'f1_weighted',\n",
      "            'neg_log_loss',          \n",
      "            'precision', \n",
      "            'precision_macro',\n",
      "            'precision_micro',\n",
      "            'precision_weighted', \n",
      "            'recall', 'recall_macro', 'recall_micro', 'recall_weighted',\n",
      "            'roc_auc'\n",
      "            ]\n",
      "\n",
      "clustering = ['adjusted_mutual_info_score',\n",
      "              'adjusted_rand_score',\n",
      "              'completeness_score',\n",
      "              'fowlkes_mallows_score',\n",
      "              'homogeneity_score',\n",
      "              'mutual_info_score',\n",
      "              'normalized_mutual_info_score',\n",
      "              'neg_log_loss',\n",
      "              'v_measure_score']\n",
      "\n",
      "regression = ['explained_variance',\n",
      "              'neg_mean_absolute_error',\n",
      "              'neg_mean_squared_error',\n",
      "              'neg_mean_squared_log_error',\n",
      "              'neg_median_absolute_error',\n",
      "              'r2']\n",
      "\n",
      "multilabel_only = ['f1_samples', 'precision_samples', 'recall_samples']\n",
      "#scorings=['accuracy', 'average_precision']\n",
      "scorings = [\"accuracy\", \"precision\", \"recall\", \"f1\"]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#svm.evaluate(mode=\"train\", cv_scorings=scorings)\n",
      "#svm.extractor.n_negative_patches *= 3\n",
      "#svm.cross_validation(cv_scorings=scorings, extract_new_features=bool(0))\n",
      "svm.evaluate_test(to_train=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print svm.test_classifier.predict(svm.data)\n",
      "print svm.labels\n",
      "\n",
      "TP = 7.0\n",
      "FP = 5.0\n",
      "FN = 2.0\n",
      "print TP / (TP+FP) \n",
      "print TP / (TP+FN) \n",
      "\n",
      "print 1 - float( np.sum(svm.test_classifier.predict(svm.data) != svm.labels)) / len(svm.data)\n",
      "#print cross_val_score(svm.test_classifier, svm.data, svm.labels)\n",
      "\n",
      "print svm.test_classifier.score(svm.data, svm.labels)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a = np.zeros(9)\n",
      "print a\n",
      "print a.reshape(1,-1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svm.dataset.create_dataset_CT()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svm.extractor.extract_feature_vects(multiple_rois=False, \n",
      "                                     save_features=False,\n",
      "                                     mode=\"transform\")\n",
      "svm.create_training_data()\n",
      "print \"\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svm.create_training_data(features=svm.extractor.features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svm.store_results()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(svm.dataset.orig_images)\n",
      "print svm.extractor.n_negatives\n",
      "print svm.extractor.n_negative_patches\n",
      "print len(svm.extractor.features)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svm.extractor.features = {}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.svm import SVC\n",
      "clf = SVC(kernel=\"linear\", C = 0.1, probability=True, random_state=42)\n",
      "#clas.cross_val_score(clf, svm.data, svm.labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.model_selection import cross_validate, cross_val_score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print cross_validate(clf, svm.data, svm.labels, scoring=scorings)#, return_train_score=False)\n",
      "print cross_val_score(clf, svm.data, svm.labels, scoring=\"accuracy\")\n",
      "print cross_val_score(clf, svm.data, svm.labels, scoring=\"precision\")\n",
      "print cross_val_score(clf, svm.data, svm.labels, scoring=\"recall\")\n",
      "print cross_val_score(clf, svm.data, svm.labels, scoring=\"f1\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clf.fit(svm.data, svm.labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ypred, yref = clf.predict(svm.data), svm.labels\n",
      "print confusion_matrix(yref, ypred)\n",
      "print classification_report(ypred, yref)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ypred, yref = svm.test_classifier.predict(svm.data), svm.labels\n",
      "print confusion_matrix(yref, ypred)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ypred, yref = svm.test_classifier.predict(svm.data), svm.labels\n",
      "print accuracy_score(svm.test_classifier.predict(svm.data), svm.labels)\n",
      "a = classification_report(svm.test_classifier.predict(svm.data), svm.labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print type(a)\n",
      "print a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ypred, yref = clf.predict(svm.data), svm.labels\n",
      "print accuracy_score(clf.predict(svm.data), svm.labels)\n",
      "a = classification_report(clf.predict(svm.data), svm.labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "svm.test_classifier = copy.copy(clf)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import fbeta_score, make_scorer, precision_recall_fscore_support\n",
      "scorer = make_scorer(\"accuracy\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print precision_recall_fscore_support(ypred, yref)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.metrics import recall_score\n",
      "print recall_score(ypred, yref)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print recall_score.__name__+\"__pll\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print svm.data.shape"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "ypred, yref = clf.predict(svm.data), svm.labels\n",
      "print accuracy_score(clf.predict(svm.data), svm.labels)\n",
      "print classification_report(clf.predict(svm.data), svm.labels)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Prohlizeni evaluation"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import data_reader as dr\n",
      "import file_manager as fm\n",
      "import copy\n",
      "import numpy as np\n",
      "import os\n",
      "import re\n",
      "\n",
      "eval_path = \"extractor_test_results/HoG/evaluation/\"\n",
      "#eval_path = \"extractor_test_results/2017-10-09__13-31-19-696000-classic/evaluation/\"\n",
      "#eval_path = \"extractor_test_results/HoG/evaluation-classic/\"\n",
      "evals = [eval_path + imgname for imgname in os.listdir(eval_path) if imgname.endswith('.json') and not ('AFFINE' in imgname)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scores = list()\n",
      "\n",
      "for eval_file in evals:\n",
      "    scores.append(dr.load_json(eval_file))\n",
      "    scores[-1][\"name\"] = fm.get_imagename(eval_file)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "best = dict()\n",
      "test_keys = [key for key in scores[0].keys() if \"test\" in key]\n",
      "\n",
      "for key in test_keys:\n",
      "    best[key] = list()\n",
      "    sorted_scores = sorted(scores, key=lambda k: np.mean(k[key]))[::-1]\n",
      "    for score in sorted_scores:\n",
      "        best[key].append((score[\"name\"], np.mean(score[key])))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def print_best(mode=\"best\", n_best=5):\n",
      "    scorings = test_keys\n",
      "    if n_best == -1:\n",
      "        n_best = len(best[scorings[0]])+1\n",
      "        \n",
      "    for scoring in scorings:\n",
      "        if mode == \"worst\":\n",
      "            print \"Nejhorsi podle \"+scoring+\": \"\n",
      "            for each in best[scoring][::-1][:n_best]:\n",
      "                print each\n",
      "        else:\n",
      "            print \"Nejlepsi podle \"+scoring+\": \"\n",
      "            for each in best[scoring][:n_best]:\n",
      "                print each"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print_best(mode=\"wors\", n_best=-1)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "name = 'extracted_features_CV_ori=8_ppc=8_cpb=4_colored'\n",
      "ppc = re.findall(r'ppc\\=\\d+', name)[0]\n",
      "ppc = int(re.findall(r'\\d+', ppc)[0])\n",
      "\n",
      "ppcs = dict()\n",
      "cpbs = dict()\n",
      "oris = dict()\n",
      "\n",
      "keys = [\"test_accuracy\"]\n",
      "keys = [\"test_recall\"]\n",
      "#keys = [\"test_precision\"]\n",
      "#keys = best.keys()\n",
      "\n",
      "for scoring in keys:\n",
      "    for i, (name, value) in enumerate(best[scoring]):\n",
      "        \n",
      "        ppc = re.findall(r'ppc\\=\\d+', name)[0]\n",
      "        ppc = int(re.findall(r'\\d+', ppc)[0])\n",
      "        cpb = re.findall(r'cpb\\=\\d+', name)[0]\n",
      "        cpb = int(re.findall(r'\\d+', cpb)[0])\n",
      "        ori = re.findall(r'ori\\=\\d+', name)[0]\n",
      "        ori = int(re.findall(r'\\d+', ori)[0])\n",
      "        \n",
      "        #if not ori = 16:\n",
      "         #   continue\n",
      "        \n",
      "        if not ppcs.has_key(ppc):\n",
      "            ppcs[ppc] = 0\n",
      "        else:\n",
      "            ppcs[ppc] += i\n",
      "\n",
      "        if not cpbs.has_key(cpb):\n",
      "            cpbs[cpb] = 0\n",
      "        else:\n",
      "            cpbs[cpb] += i  \n",
      "\n",
      "        if not oris.has_key(ori):\n",
      "            oris[ori] = 0\n",
      "        else:\n",
      "            oris[ori] += i\n",
      "            \n",
      "print len(best[best.keys()[0]])        \n",
      "print oris\n",
      "print ppcs\n",
      "print cpbs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import OrderedDict\n",
      "\n",
      "oris_sorted_by_value = OrderedDict(sorted(oris.items(), key=lambda x: x[1]))\n",
      "ppcs_sorted_by_value = OrderedDict(sorted(ppcs.items(), key=lambda x: x[1]))\n",
      "cpbs_sorted_by_value = OrderedDict(sorted(cpbs.items(), key=lambda x: x[1]))\n",
      "\n",
      "print keys\n",
      "print \"ori: \", oris_sorted_by_value\n",
      "print \"ppc: \", ppcs_sorted_by_value\n",
      "print \"cpb: \", cpbs_sorted_by_value"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# u medianu nejlepsi 16, 8, 3\n",
      "# u bilatelar: recall nejhorsi: 1\n",
      "#              precision nejhorsi: 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}